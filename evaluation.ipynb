{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# your model focuses on during captioning\n",
    "# from nltk.translate.bleu_score import corpus_bleu\n",
    "# from nltk.translate.meteor_score import meteor_score\n",
    "# from rouge import Rouge\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import configparser\n",
    "import re\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "import json\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"config.ini\")\n",
    "\n",
    "#importing local module \n",
    "from models.subclasses import *\n",
    "from models.utilities import *\n",
    "from models.predict import *\n",
    "from models.train_utils import *\n",
    "from models.evaluation_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_plot = load_loss()\n",
    "print(len(loss_plot))\n",
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = int(config['config']['units'])\n",
    "embedding_dim = int(config['config']['embedding_dim'])\n",
    "\n",
    "vocabulary_size = int(config['config']['vocabulary_size'])\n",
    "\n",
    "\n",
    "use_glove = bool(config['config']['use_glove'])\n",
    "glove_dim = int(config['config']['glove_dim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "val_image_paths, image_path_to_caption_val = import_files(shuffle= False, method = \"val\")\n",
    "\n",
    "val_captions = []\n",
    "img_name_vector_val = []\n",
    "for image_path in val_image_paths:\n",
    "  caption_list = image_path_to_caption_val[image_path]\n",
    "  if len(caption_list)!=5:\n",
    "    caption_list = caption_list[:5]\n",
    "  val_captions.extend(caption_list)\n",
    "  img_name_vector_val.extend([image_path] * len(caption_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "train_image_paths, image_path_to_caption_train = import_files(shuffle= False, method = \"train\")\n",
    "\n",
    "train_captions = []\n",
    "img_name_vector_train = []\n",
    "for image_path in train_image_paths:\n",
    "  caption_list = image_path_to_caption_train[image_path]\n",
    "  if len(caption_list)!=5:\n",
    "    caption_list = caption_list[:5]\n",
    "  train_captions.extend(caption_list)\n",
    "  img_name_vector_train.extend([image_path] * len(caption_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, vocabulary, _ = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index_val, index_to_word_val, tokenizer_val, cap_vector_val = tokenization(val_captions, max_length, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index_train, index_to_word_train, tokenizer_train, cap_vector_train = tokenization(train_captions, max_length, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1581"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_glove_path = f\"./dataset/glove.6B/new_glove.6B.{glove_dim}d.pkl\"\n",
    "tuned_glove = pickle.load(open(new_glove_path, \"rb\"))\n",
    "len(tuned_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 401581 word vectors.\n",
      "Converted 4543 words (118 misses)\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "if use_glove:\n",
    "    glove_path = f\"./dataset/glove.6B/glove.6B.{glove_dim}d.txt\"\n",
    "\n",
    "    with open(glove_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    embeddings_index.update(tuned_glove)\n",
    "\n",
    "    print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "\n",
    "    vocabulary = tokenizer_train.get_vocabulary()\n",
    "    word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "    num_tokens = len(vocabulary) +2\n",
    "    embedding_dim = 100\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "\n",
    "    # Prepare embedding matrix\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            # This includes the representation for \"padding\" and \"OOV\"\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "if use_glove:\n",
    "    decoder = RNN_Decoder(embedding_dim, units, num_tokens, embedding_matrix)\n",
    "else:\n",
    "    decoder = RNN_Decoder(embedding_dim, units, tokenizer_train.vocabulary_size(), None)\n",
    "image_features_extract_model = get_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:39<00:00,  3.97it/s]\n"
     ]
    }
   ],
   "source": [
    "img_name_val, cap_val = split_data(img_name_vector_val, cap_vector_val ,\n",
    "                                    image_features_extract_model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cap_vector_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1d41b69c910>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getHypotheses(img_name_val, encoder, decoder,\n",
    "#                     image_features_extract_model,word_to_index_train,\n",
    "#                     index_to_word_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_name_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeResultFile(img_name_val, encoder, decoder,\n",
    "                  image_features_extract_model, word_to_index_train,\n",
    "                  index_to_word_train):\n",
    "\n",
    "    result_list = []\n",
    "    for i in range(0, len(img_name_val), 5):\n",
    "        id = img_name_vector_val[i].split(\"val2017\\\\\")[1].split(\".\")[0]\n",
    "        cap = predict(img_name_val[i], encoder, decoder, image_features_extract_model,\n",
    "                         word_to_index_train, index_to_word_train)\n",
    "        \n",
    "        if cap[-1] == \"<end>\":\n",
    "            cap.remove(\"<end>\")\n",
    "             \n",
    "        cap = ' '.join(cap)\n",
    "        \n",
    "        temp = {\"image_id\": int(id.lstrip('0')), \"caption\": cap}\n",
    "        \n",
    "        result_list.append(temp)\n",
    "    \n",
    "    with open('dataset\\coco\\\\result\\\\result.json', 'w') as outfile:\n",
    "        json.dump(result_list, outfile,sort_keys=True)\n",
    "    \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = 'dataset\\coco\\\\val\\captions_val2017.json'\n",
    "results_file = 'dataset\\coco\\\\result\\\\result.json'\n",
    "\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'r') as f:\n",
    "        result = json.load(f)\n",
    "else:\n",
    "    result_a = makeResultFile(img_name_val, encoder, decoder,\n",
    "                    image_features_extract_model, word_to_index_train,\n",
    "                    index_to_word_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.19s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 53817, 'reflen': 50921, 'guess': [53817, 48838, 43960, 39659], 'correct': [15268, 2255, 150, 19]}\n",
      "ratio: 1.0568724102040208\n",
      "Bleu_1: 0.284\n",
      "Bleu_2: 0.114\n",
      "Bleu_3: 0.035\n",
      "Bleu_4: 0.012\n",
      "computing METEOR score...\n",
      "METEOR: 0.068\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.202\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.014\n",
      "computing SPICE score...\n",
      "SPICE: 0.013\n",
      "Bleu_1: 0.284\n",
      "Bleu_2: 0.114\n",
      "Bleu_3: 0.035\n",
      "Bleu_4: 0.012\n",
      "METEOR: 0.068\n",
      "ROUGE_L: 0.202\n",
      "CIDEr: 0.014\n",
      "SPICE: 0.013\n"
     ]
    }
   ],
   "source": [
    "# create coco object and coco_result object\n",
    "coco = COCO(annotation_file)\n",
    "coco_result = coco.loadRes(results_file)\n",
    "\n",
    "# create coco_eval object by taking coco and coco_result\n",
    "coco_eval = COCOEvalCap(coco, coco_result)\n",
    "\n",
    "coco_eval.evaluate()\n",
    "\n",
    "# print output evaluation scores\n",
    "for metric, score in coco_eval.eval.items():\n",
    "    print(f'{metric}: {score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgIds = coco_eval.params['image_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gts = {}\n",
    "res = {}\n",
    "for imgId in imgIds:\n",
    "    gts[imgId] = coco.imgToAnns[imgId]\n",
    "    res[imgId] = coco_result.imgToAnns[imgId]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
    "tokenizer = PTBTokenizer()\n",
    "gts  = tokenizer.tokenize(gts)\n",
    "res = tokenizer.tokenize(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gts.keys() - res.keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coco_result.imgToAnns[179765])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coco.imgToAnns[179765])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in imgIds:\n",
    "    hypo = res[id]\n",
    "    ref = gts[id]\n",
    "    # print(len(hypo))\n",
    "    if(len(hypo)!= 1):\n",
    "        print(len(hypo))\n",
    "        print(id)\n",
    "    # print(ref)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(' '.join([tf.compat.as_text(index_to_word_val(j).numpy())\n",
    "                         for j in cap_val[i] if j not in [0]]).split('<start>')[1].split('<end>')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = []\n",
    "list_of_references = []\n",
    "\n",
    "for i in range(len(img_name_val)):\n",
    "    references.append(' '.join([tf.compat.as_text(index_to_word_val(j).numpy())\n",
    "                                for j in cap_val[i] if j not in [0]]).split('<start>')[1].split('<end>')[0])\n",
    "    \n",
    "for i in range(0, len(img_name_val), 5):\n",
    "    list_of_references.append(references[i:i+5])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_of_references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cap_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_references[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_hypotheses = getHypotheses(img_name_val, encoder, decoder,\n",
    "                    image_features_extract_model,word_to_index_train,\n",
    "                    index_to_word_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_of_hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_all(len(img_name_val), list_of_references, list_of_hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions on the test set\n",
    "# rid = np.random.randint(0, len(img_name_val))\n",
    "# image = img_name_val[rid]\n",
    "num = np.random.randint(0, len(img_name_val)/5)\n",
    "test_references = []\n",
    "for i in range(5):\n",
    "   test_references.append(list_of_references[num][i].split())\n",
    "\n",
    "image = img_name_val[num*5]\n",
    "# real_caption = ' '.join([tf.compat.as_text(index_to_word(i).numpy())\n",
    "#                          for i in cap_val[rid] if i not in [0]]).split()\n",
    "\n",
    "result, attention_plot = evaluate(image, encoder, decoder, image_features_extract_model,\n",
    "                                    word_to_index_train, index_to_word_train)\n",
    "\n",
    "if result[-1] == \"<end>\":\n",
    "    result.remove(\"<end>\")\n",
    "\n",
    "# if real_caption[0] == \"<start>\":\n",
    "#     real_caption.remove(\"<start>\")\n",
    "\n",
    "rouge = Rouge()\n",
    "list_of_hypotheses = [result]\n",
    "\n",
    "print('Real Caption:', list_of_references[num][0])\n",
    "print('Prediction Caption:', ' '.join(result))\n",
    "print('Meteor: %f' % meteor_score(test_references, result))\n",
    "print('Rouge: \\n', rouge.get_scores(refs=list_of_references[num][0], hyps=' '.join(result), avg=True))\n",
    "print('BLEU-1: %f' % corpus_bleu([test_references], list_of_hypotheses, weights=(1.0, 0, 0, 0)))\n",
    "print('BLEU-2: %f' % corpus_bleu([test_references], list_of_hypotheses, weights=(0.5, 0.5, 0, 0)))\n",
    "print('BLEU-3: %f' % corpus_bleu([test_references], list_of_hypotheses, weights=(0.3, 0.3, 0.3, 0)))\n",
    "print('BLEU-4: %f' % corpus_bleu([test_references], list_of_hypotheses, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "plot_attention(image, result, attention_plot)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fefb53ccb8826f5d221933f4e956154857a1cf678d9c734061c64f9df1cd0fd7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('uvapp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
