{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"config.ini\")\n",
    "\n",
    "# from test import *\n",
    "# from models.utilities import *\n",
    "from models.subclasses import *\n",
    "from models.predict import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static values\n",
    "PATH = 'testVideos/IMG_9367.MOV'\n",
    "\n",
    "test_image, _ = load_image(\"testVideos/35506150_cbdb630f4f.jpg\")\n",
    "\n",
    "test_image2, _ = load_image(\"testVideos/IMG_3004.jpg\")\n",
    "\n",
    "#how many frame to play until pause\n",
    "SHOW_FRAME = 60\n",
    "\n",
    "units = int(config['config']['units'])\n",
    "embedding_dim = int(config['config']['embedding_dim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_train, cap_train, img_name_val, cap_val, vocabulary, tokens_shape = load_dataset()\n",
    "word_to_index, index_to_word = index_vocab(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (<keras.layers.preprocessing.string_lookup.StringLookup object at 0x000001B00FCDE700>) with an unsupported type (<class 'keras.layers.preprocessing.string_lookup.StringLookup'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hazem\\Desktop\\collage\\Graduation project\\Unlimited vision app\\app.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hazem/Desktop/collage/Graduation%20project/Unlimited%20vision%20app/app.ipynb#ch0000011?line=0'>1</a>\u001b[0m temp \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mconvert_to_tensor(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hazem/Desktop/collage/Graduation%20project/Unlimited%20vision%20app/app.ipynb#ch0000011?line=1'>2</a>\u001b[0m     word_to_index\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hazem/Desktop/collage/Graduation%20project/Unlimited%20vision%20app/app.ipynb#ch0000011?line=2'>3</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hazem/Desktop/collage/Graduation%20project/Unlimited%20vision%20app/app.ipynb#ch0000011?line=3'>4</a>\u001b[0m temp(\u001b[39m'\u001b[39m\u001b[39m<start>\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\uvapp\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/hazem/anaconda3/envs/uvapp/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///c%3A/Users/hazem/anaconda3/envs/uvapp/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> <a href='file:///c%3A/Users/hazem/anaconda3/envs/uvapp/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=152'>153</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/hazem/anaconda3/envs/uvapp/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=153'>154</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/hazem/anaconda3/envs/uvapp/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=154'>155</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\uvapp\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:106\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/hazem/anaconda3/envs/uvapp/lib/site-packages/tensorflow/python/framework/constant_op.py?line=103'>104</a>\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    <a href='file:///c%3A/Users/hazem/anaconda3/envs/uvapp/lib/site-packages/tensorflow/python/framework/constant_op.py?line=104'>105</a>\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> <a href='file:///c%3A/Users/hazem/anaconda3/envs/uvapp/lib/site-packages/tensorflow/python/framework/constant_op.py?line=105'>106</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Attempt to convert a value (<keras.layers.preprocessing.string_lookup.StringLookup object at 0x000001B00FCDE700>) with an unsupported type (<class 'keras.layers.preprocessing.string_lookup.StringLookup'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "temp = tf.convert_to_tensor(\n",
    "    word_to_index\n",
    ")\n",
    "temp('<start>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, len(vocabulary))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1b05829caf0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "_, _, image_features_extract_model = load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all(frame):\n",
    "    img = tf.keras.layers.Resizing(224, 224)(frame)\n",
    "    img = tf.keras.applications.resnet50.preprocess_input(img)\n",
    "    result = predict_image(img, encoder, decoder,\n",
    "            image_features_extract_model,\n",
    "            word_to_index, index_to_word)\n",
    "    temp = Image.fromarray(frame, 'RGB')\n",
    "#     temp.show()\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'man',\n",
       " 'lounging',\n",
       " 'on',\n",
       " 'one',\n",
       " 'of',\n",
       " 'greenery',\n",
       " 'and',\n",
       " 'about',\n",
       " 'to',\n",
       " 'relax',\n",
       " 'on',\n",
       " 'a',\n",
       " 'seat',\n",
       " '<end>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(test_image, encoder, decoder,\n",
    "            image_features_extract_model,\n",
    "            word_to_index, index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'grassy',\n",
       " 'field',\n",
       " 'with',\n",
       " 'many',\n",
       " 'purple',\n",
       " 'dots',\n",
       " 'on',\n",
       " 'top',\n",
       " 'of',\n",
       " 'a',\n",
       " 'big',\n",
       " 'green',\n",
       " 'field.',\n",
       " '<end>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image(test_image2, encoder, decoder,\n",
    "            image_features_extract_model,\n",
    "            word_to_index, index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statred capturing\n",
      "['a', 'picture', 'of', 'a', 'restaurant', 'is', 'blue', 'lamps', 'in', 'the', 'rain', '<end>']\n",
      "Time taken for 1 image 0.6970 sec\n",
      "\n",
      "['a', 'potted', 'plant', 'in', 'front', 'of', 'pedestal', 'in', 'a', 'sink.', '<end>']\n",
      "Time taken for 1 image 0.6160 sec\n",
      "\n",
      "['a', 'green', 'bench', 'in', 'a', 'courtyard', 'looking', 'hat', 'sitting', 'gym', '<end>']\n",
      "Time taken for 1 image 0.6220 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#loop over all frames  'space' = next frame | 'q' = quit\n",
    "cap = cv2.VideoCapture(PATH)\n",
    "if (cap.isOpened()== False):\n",
    "    print(\"Error opening video stream or file\")\n",
    "else:\n",
    "    print(\"Statred capturing\")\n",
    "\n",
    "total_frames = 0\n",
    "while cap.isOpened():\n",
    "    HasFrames, frame = cap.read()\n",
    "    #if vidoe is not done do\n",
    "    if HasFrames:\n",
    "        total_frames += 1\n",
    "        cv2.imshow('Video', frame)\n",
    "        #when you reach the pause frame do\n",
    "        if((total_frames % SHOW_FRAME) == 0):\n",
    "            #Caption 'frame'\n",
    "            start = time.time()\n",
    "            # print(frame.shape)\n",
    "            predict_all(frame)\n",
    "            print(f'Time taken for 1 image {time.time()-start:.4f} sec\\n')\n",
    "            #press 'E' to get next frame\n",
    "            if(cv2.waitKey(5000) == ord('e')):\n",
    "                continue\n",
    "\n",
    "        if(cv2.waitKey(25) == ord('q')):\n",
    "            break    \n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a0855005469a133b1a4fed7e6a9d3300657ffc4061414113a9f065e922aa0ee4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('uvapp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
