{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# your model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "#app imports\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download caption annotation files\n",
    "annotation_file = 'dataset\\coco\\\\tarin\\captions_train2017.json'\n",
    "annotation_folder = '\\dataset\\coco\\\\annotations_trainval2017\\\\annotations\\\\'\n",
    "if not os.path.exists(os.path.abspath('.') + annotation_folder):\n",
    "  annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
    "                                           cache_subdir=os.path.abspath('.'),\n",
    "                                           origin='http://images.cocodataset.org/annotations/annotations_trainval2017.zip',\n",
    "                                           extract=True)\n",
    "  annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2017.json'\n",
    "  os.remove(annotation_zip)\n",
    "\n",
    "# Download image files\n",
    "image_folder = '\\dataset\\coco\\\\tarin\\images\\\\'\n",
    "if not os.path.exists(os.path.abspath('.') + image_folder):\n",
    "  image_zip = tf.keras.utils.get_file('train2017.zip',\n",
    "                                      cache_subdir=os.path.abspath('.'),\n",
    "                                      origin='http://images.cocodataset.org/zips/train2017.zip',\n",
    "                                      extract=True)\n",
    "  PATH = os.path.dirname(image_zip) + image_folder\n",
    "  os.remove(image_zip)\n",
    "else:\n",
    "  PATH = os.path.abspath('.') + image_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group all captions together having the same image ID.\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for val in annotations['annotations']:\n",
    "  caption = f\"<start> {val['caption']} <end>\"\n",
    "  image_path = PATH +'%012d.jpg' % (val['image_id'])\n",
    "  #image_path.replace(\"\\\\\",\"/\") #windows\n",
    "  image_path_to_caption[image_path].append(caption)\n",
    "\n",
    "#image_path_to_caption = [item.replace(\"\\\\\",\"/\") for item in image_path_to_caption] #windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "image_paths = list(image_path_to_caption.keys())\n",
    "random.shuffle(image_paths)\n",
    "\n",
    "# Select the first 6000 image_paths from the shuffled set.\n",
    "# Approximately each image id has 5 captions associated with it, so that will\n",
    "# lead to 30,000 examples.\n",
    "train_image_paths = image_paths[:10]\n",
    "#train_image_paths = [item.replace(\"\\\\\",\"/\") for item in train_image_paths] #windows\n",
    "print(len(train_image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions = []\n",
    "img_name_vector = []\n",
    "\n",
    "for image_path in train_image_paths:\n",
    "  caption_list = image_path_to_caption[image_path]\n",
    "  train_captions.extend(caption_list)\n",
    "  img_name_vector.extend([image_path] * len(caption_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.keras.layers.Resizing(299, 299)(img)\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique images\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# Feel free to change batch_size according to your system configuration\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(\n",
    "  load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(64)\n",
    "\n",
    "for img, path in image_dataset:\n",
    "    batch_features = image_features_extract_model(img) #Kernal dies here :D\n",
    "    batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        np.save(path_of_feature, bf.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_dataset = tf.data.Dataset.from_tensor_slices(train_captions)\n",
    "\n",
    "# We will override the default standardization of TextVectorization to preserve\n",
    "# \"<>\" characters, so we preserve the tokens for the <start> and <end>.\n",
    "def standardize(inputs):\n",
    "  inputs = tf.strings.lower(inputs)\n",
    "  return tf.strings.regex_replace(inputs,\n",
    "                                  r\"!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~\", \"\")\n",
    "\n",
    "# Max word count for a caption.\n",
    "max_length = 50\n",
    "# Use the top 5000 words for a vocabulary.\n",
    "vocabulary_size = 5000\n",
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocabulary_size,\n",
    "    standardize=standardize,\n",
    "    output_sequence_length=max_length)\n",
    "# Learn the vocabulary from the caption data.\n",
    "tokenizer.adapt(caption_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenized vectors\n",
    "cap_vector = caption_dataset.map(lambda x: tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: (None,), types: tf.int64>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_vector.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings for words to indices and indicies to words.\n",
    "word_to_index = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary())\n",
    "index_to_word = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary(),\n",
    "    invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "for img, cap in zip(img_name_vector, cap_vector):\n",
    "  img_to_cap_vector[img].append(cap)\n",
    "\n",
    "# Create training and validation sets using an 80-20 split randomly.\n",
    "img_keys = list(img_to_cap_vector.keys())\n",
    "random.shuffle(img_keys)\n",
    "\n",
    "slice_index = int(len(img_keys)*0.8)\n",
    "img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
    "\n",
    "img_name_train = []\n",
    "cap_train = []\n",
    "for imgt in img_name_train_keys:\n",
    "  capt_len = len(img_to_cap_vector[imgt])\n",
    "  img_name_train.extend([imgt] * capt_len)\n",
    "  cap_train.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "img_name_val = []\n",
    "cap_val = []\n",
    "for imgv in img_name_val_keys:\n",
    "  capv_len = len(img_to_cap_vector[imgv])\n",
    "  img_name_val.extend([imgv] * capv_len)\n",
    "  cap_val.extend(img_to_cap_vector[imgv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40, 10, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these parameters according to your system's configuration\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 224\n",
    "units = 2048\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def map_func(img_name, cap):\n",
    "  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "  return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int64]),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, features, hidden):\n",
    "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "    # hidden shape == (batch_size, hidden_size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "    # attention_hidden_layer shape == (batch_size, 64, units)\n",
    "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                         self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # score shape == (batch_size, 64, 1)\n",
    "    # This gives you an unnormalized score for each image feature.\n",
    "    score = self.V(attention_hidden_layer)\n",
    "\n",
    "    # attention_weights shape == (batch_size, 64, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * features\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units, vocab_size):\n",
    "    super(RNN_Decoder, self).__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "  def call(self, x, features, hidden):\n",
    "    # defining attention as a separate model\n",
    "    context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # shape == (batch_size, max_length, hidden_size)\n",
    "    x = self.fc1(output)\n",
    "\n",
    "    # x shape == (batch_size * max_length, hidden_size)\n",
    "    x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size * max_length, vocab)\n",
    "    x = self.fc2(x)\n",
    "\n",
    "    return x, state, attention_weights\n",
    "\n",
    "  def reset_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, tokenizer.vocabulary_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_epoch = 0\n",
    "# if ckpt_manager.latest_checkpoint:\n",
    "#   start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "#   # restoring the latest checkpoint in checkpoint_path\n",
    "#   ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "  loss = 0\n",
    "\n",
    "  print(\"Target shape:  \",target.shape[0])\n",
    "  print(target)\n",
    "  # initializing the hidden state for each batch\n",
    "  # because the captions are not related from image to image\n",
    "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "  dec_input = tf.expand_dims([word_to_index('<start>')] * target.shape[0], 1)\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "      features = encoder(img_tensor)\n",
    "\n",
    "      for i in range(1, target.shape[1]):\n",
    "          # passing the features through the decoder\n",
    "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "          loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "          # using teacher forcing\n",
    "          dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "  print(\"Ended gradiant tape\")\n",
    "\n",
    "  total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "  print(\"total_loss\")\n",
    "  print(total_loss)\n",
    "  print(\"Type : \",type(total_loss))\n",
    "\n",
    "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  \n",
    "  print(\"trainable_variables\")\n",
    "  print(trainable_variables)\n",
    "  print(\"Type : \",type(trainable_variables))\n",
    "\n",
    "  gradients = tape.gradient(loss, trainable_variables)\n",
    "  \n",
    "  print(\"gradients\")\n",
    "  print(gradients)\n",
    "  print(\"Type : \",type(gradients))\n",
    "  \n",
    "  print(\"zipped\")\n",
    "  print(zip(gradients, trainable_variables))\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, trainable_variables)) # <--- breaks here \n",
    "\n",
    "  print(\"returnd\")\n",
    "  return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder.build()\n",
    "# decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Target shape:   tf.Tensor(\n",
      "[  3  12  31   6 170 166 196  34 164  62 120   4   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0], shape=(50,), dtype=int64)\n",
      "(40, 50)\n"
     ]
    }
   ],
   "source": [
    "# print(dataset)\n",
    "for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "    print(type(target))\n",
    "    print(type(target[0]))\n",
    "    print(\"Target shape:  \", target[2])\n",
    "    print(target.shape)\n",
    "    # print(\"Target:  \", target)\n",
    "    # print(\"Target shape:  \", target.get_shape())\n",
    "    # print(\"image tensor:  \", img_tensor)\n",
    "    # print(\"image tensor shape:  \", img_tensor.get_shape())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target shape:   40\n",
      "Tensor(\"target:0\", shape=(40, 50), dtype=int64)\n",
      "Ended gradiant tape\n",
      "total_loss\n",
      "Tensor(\"truediv:0\", shape=(), dtype=float32)\n",
      "Type :  <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "trainable_variables\n",
      "[<tf.Variable 'cnn__encoder/dense/kernel:0' shape=(2048, 224) dtype=float32>, <tf.Variable 'cnn__encoder/dense/bias:0' shape=(224,) dtype=float32>, <tf.Variable 'rnn__decoder/embedding/embeddings:0' shape=(198, 224) dtype=float32>, <tf.Variable 'rnn__decoder/gru/gru_cell/kernel:0' shape=(448, 6144) dtype=float32>, <tf.Variable 'rnn__decoder/gru/gru_cell/recurrent_kernel:0' shape=(2048, 6144) dtype=float32>, <tf.Variable 'rnn__decoder/gru/gru_cell/bias:0' shape=(2, 6144) dtype=float32>, <tf.Variable 'rnn__decoder/dense_1/kernel:0' shape=(2048, 2048) dtype=float32>, <tf.Variable 'rnn__decoder/dense_1/bias:0' shape=(2048,) dtype=float32>, <tf.Variable 'rnn__decoder/dense_2/kernel:0' shape=(2048, 198) dtype=float32>, <tf.Variable 'rnn__decoder/dense_2/bias:0' shape=(198,) dtype=float32>, <tf.Variable 'rnn__decoder/bahdanau_attention/dense_3/kernel:0' shape=(224, 2048) dtype=float32>, <tf.Variable 'rnn__decoder/bahdanau_attention/dense_3/bias:0' shape=(2048,) dtype=float32>, <tf.Variable 'rnn__decoder/bahdanau_attention/dense_4/kernel:0' shape=(2048, 2048) dtype=float32>, <tf.Variable 'rnn__decoder/bahdanau_attention/dense_4/bias:0' shape=(2048,) dtype=float32>, <tf.Variable 'rnn__decoder/bahdanau_attention/dense_5/kernel:0' shape=(2048, 1) dtype=float32>, <tf.Variable 'rnn__decoder/bahdanau_attention/dense_5/bias:0' shape=(1,) dtype=float32>]\n",
      "Type :  <class 'list'>\n",
      "gradients\n",
      "[<tf.Tensor 'gradient_tape/cnn__encoder/dense/Tensordot/MatMul/MatMul:0' shape=(2048, 224) dtype=float32>, <tf.Tensor 'gradient_tape/cnn__encoder/dense/BiasAdd/BiasAddGrad:0' shape=(224,) dtype=float32>, <tensorflow.python.framework.indexed_slices.IndexedSlices object at 0x00000141F7F5F850>, <tf.Tensor 'AddN_26:0' shape=(448, 6144) dtype=float32>, <tf.Tensor 'AddN_27:0' shape=(2048, 6144) dtype=float32>, <tf.Tensor 'AddN_30:0' shape=(2, 6144) dtype=float32>, <tf.Tensor 'AddN_5:0' shape=(2048, 2048) dtype=float32>, <tf.Tensor 'AddN_31:0' shape=(2048,) dtype=float32>, <tf.Tensor 'AddN_32:0' shape=(2048, 198) dtype=float32>, <tf.Tensor 'AddN_33:0' shape=(198,) dtype=float32>, <tf.Tensor 'AddN_34:0' shape=(224, 2048) dtype=float32>, <tf.Tensor 'AddN_35:0' shape=(2048,) dtype=float32>, <tf.Tensor 'AddN_28:0' shape=(2048, 2048) dtype=float32>, <tf.Tensor 'AddN_36:0' shape=(2048,) dtype=float32>, <tf.Tensor 'AddN_37:0' shape=(2048, 1) dtype=float32>, <tf.Tensor 'AddN_38:0' shape=(1,) dtype=float32>]\n",
      "Type :  <class 'list'>\n",
      "zipped\n",
      "<zip object at 0x00000143E979B280>\n",
      "returnd\n",
      "Target shape:   40\n",
      "Tensor(\"target:0\", shape=(40, 50), dtype=int64)\n",
      "Ended gradiant tape\n",
      "total_loss\n",
      "Tensor(\"truediv:0\", shape=(), dtype=float32)\n",
      "Type :  <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "trainable_variables\n",
      "[<tf.Variable 'cnn__encoder/dense/kernel:0' shape=(2048, 224) dtype=float32>, <tf.Variable 'cnn__encoder/dense/bias:0' shape=(224,) dtype=float32>, <tf.Variable 'rnn__decoder/embedding/embeddings:0' shape=(198, 224) dtype=float32>, <tf.Variable 'rnn__decoder/gru/gru_cell/kernel:0' shape=(448, 6144) dtype=float32>, <tf.Variable 'rnn__decoder/gru/gru_cell/recurrent_kernel:0' shape=(2048, 6144) dtype=float32>, <tf.Variable 'rnn__decoder/gru/gru_cell/bias:0' shape=(2, 6144) dtype=float32>, <tf.Variable 'rnn__decoder/dense_1/kernel:0' shape=(2048, 2048) dtype=float32>, <tf.Variable 'rnn__decoder/dense_1/bias:0' shape=(2048,) dtype=float32>, <tf.Variable 'rnn__decoder/dense_2/kernel:0' shape=(2048, 198) dtype=float32>, <tf.Variable 'rnn__decoder/dense_2/bias:0' shape=(198,) dtype=float32>, <tf.Variable 'rnn__decoder/bahdanau_attention/dense_3/kernel:0' shape=(224, 2048) dtype=float32>, <tf.Variable 'rnn__decoder/bahdanau_attention/dense_3/bias:0' shape=(2048,) dtype=float32>, <tf.Variable 'rnn__decoder/bahdanau_attention/dense_4/kernel:0' shape=(2048, 2048) dtype=float32>, <tf.Variable 'rnn__decoder/bahdanau_attention/dense_4/bias:0' shape=(2048,) dtype=float32>, <tf.Variable 'rnn__decoder/bahdanau_attention/dense_5/kernel:0' shape=(2048, 1) dtype=float32>, <tf.Variable 'rnn__decoder/bahdanau_attention/dense_5/bias:0' shape=(1,) dtype=float32>]\n",
      "Type :  <class 'list'>\n",
      "gradients\n",
      "[<tf.Tensor 'gradient_tape/cnn__encoder/dense/Tensordot/MatMul/MatMul:0' shape=(2048, 224) dtype=float32>, <tf.Tensor 'gradient_tape/cnn__encoder/dense/BiasAdd/BiasAddGrad:0' shape=(224,) dtype=float32>, <tensorflow.python.framework.indexed_slices.IndexedSlices object at 0x0000014426C86D00>, <tf.Tensor 'AddN_26:0' shape=(448, 6144) dtype=float32>, <tf.Tensor 'AddN_27:0' shape=(2048, 6144) dtype=float32>, <tf.Tensor 'AddN_30:0' shape=(2, 6144) dtype=float32>, <tf.Tensor 'AddN_5:0' shape=(2048, 2048) dtype=float32>, <tf.Tensor 'AddN_31:0' shape=(2048,) dtype=float32>, <tf.Tensor 'AddN_32:0' shape=(2048, 198) dtype=float32>, <tf.Tensor 'AddN_33:0' shape=(198,) dtype=float32>, <tf.Tensor 'AddN_34:0' shape=(224, 2048) dtype=float32>, <tf.Tensor 'AddN_35:0' shape=(2048,) dtype=float32>, <tf.Tensor 'AddN_28:0' shape=(2048, 2048) dtype=float32>, <tf.Tensor 'AddN_36:0' shape=(2048,) dtype=float32>, <tf.Tensor 'AddN_37:0' shape=(2048, 1) dtype=float32>, <tf.Tensor 'AddN_38:0' shape=(1,) dtype=float32>]\n",
      "Type :  <class 'list'>\n",
      "zipped\n",
      "<zip object at 0x000001443E76AF80>\n",
      "returnd\n",
      "Epoch 1 Batch 0 Loss 1.1567\n",
      "Epoch 1 Loss inf\n",
      "Time taken for 1 epoch 69.85 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.0524\n",
      "Epoch 2 Loss inf\n",
      "Time taken for 1 epoch 0.49 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 2.7461\n",
      "Epoch 3 Loss inf\n",
      "Time taken for 1 epoch 0.50 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.0033\n",
      "Epoch 4 Loss inf\n",
      "Time taken for 1 epoch 0.49 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.0311\n",
      "Epoch 5 Loss inf\n",
      "Time taken for 1 epoch 0.47 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "      ckpt_manager.save()\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ValueError: Shapes (2048, 512) and (512, 224) are incompatible with\n",
    "    # Feel free to change these parameters according to your system's configuration\n",
    "    # BATCH_SIZE = 64\n",
    "    # BUFFER_SIZE = 1000\n",
    "    # embedding_dim = 512\n",
    "    # units = 512\n",
    "    # num_steps = len(img_name_train) // BATCH_SIZE\n",
    "    # # Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "    # # These two variables represent that vector shape\n",
    "    # features_shape = 2048\n",
    "    # attention_features_shape = 64\n",
    "\n",
    "#-------------------------------------\n",
    "# ValueError: Shapes (2048, 224) and (512, 224) are incompatible with\n",
    "\n",
    "\n",
    "    # Feel free to change these parameters according to your system's configuration\n",
    "    # BATCH_SIZE = 64\n",
    "    # BUFFER_SIZE = 1000\n",
    "    # embedding_dim = 224\n",
    "    # units = 512\n",
    "    # num_steps = len(img_name_train) // BATCH_SIZE\n",
    "    # # Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "    # # These two variables represent that vector shape\n",
    "    # features_shape = 2048\n",
    "    # attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEWCAYAAABIVsEJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUPUlEQVR4nO3de7Bd5X3e8e+DhGVsEq4CYwlZEJSkIqlxZxfXiTtDDOaSlMs4tMZ1HTV1hkkamjqEFLl4YoyZKRA7MMS4rWonVu3aQOk4UUtiLLBJPI3H5gjji+wokgUUZDDC3KxiczG//rGXyuawhY7erX22js/3M7PmrPWud6/1e3Vm9Jy13r3XTlUhSdKe2m/SBUiS5iYDRJLUxACRJDUxQCRJTQwQSVITA0SS1MQAkeagJJcm+cSk69D8ZoBIu5HkniSnTOC8H0vydJIdSR5Jsj7JzzYcZyL168efASLt266qqgOBpcBDwMcmW470PANEapRkUZJrknynW65Jsqjbd3iS/5Xkse7q4QtJ9uv2XZxkW5LvJ9mU5OTdnauqngQ+CfzcLmo5K8nG7ny3J/l7XfvHgWXA/+yuZP7d3hq/ZIBI7S4B/hFwAvBa4ETgPd2+3wPuBxYDRwL/HqgkPwNcAPzDqvoJ4DTgnt2dKMmBwNuBrwzZ99PAp4B3def7C/qB8bKqegfwf4Azq+rAqrqqcazSixggUru3A5dV1UNVtR14H/CObt8zwFHAa6rqmar6QvUfPPcjYBGwMsn+VXVPVX37Jc5xUZLHgC3AgcC/HNLnrcDNVbW+qp4BPgAcAPzC6EOUds0Akdq9Grh3YPverg3gD+n/p//ZJFuTrAaoqi30rxQuBR5Kcn2SV7NrH6iqg6vqVVV11i7C5gV1VNVzwH3AkrZhSTNjgEjtvgO8ZmB7WddGVX2/qn6vqo4FzgIu3DnXUVWfrKo3dq8t4Mq9WUeSAEcD27omH7mtsTBApJnZP8nLB5aF9Ocd3pNkcZLDgT8APgGQ5J8kOa77z/xx+reunkvyM0ne1E22/xD4AfDciLXdCPxKkpOT7E9//uUp4G+6/d8Fjh3xHNKLGCDSzPwF/f/sdy6XApcDU8DXgK8Dd3ZtACuAW4EdwBeBD1fV5+nPf1wBPAw8CBwBvHuUwqpqE/AvgD/ujnsm/Unzp7su/4F+0D2W5KJRziUNil8oJUlq4RWIJKmJASJJamKASJKaGCCSpCYLJ13AbDr88MNr+fLlky5DkuaUDRs2PFxVi6e3z6sAWb58OVNTU5MuQ5LmlCT3Dmv3FpYkqYkBIklqYoBIkpoYIJKkJgaIJKmJASJJamKASJKaGCCSpCYGiCSpiQEiSWpigEiSmhggkqQmBogkqYkBIklqYoBIkpoYIJKkJgaIJKmJASJJamKASJKaGCCSpCYGiCSpiQEiSWpigEiSmhggkqQmBogkqclEAyTJ6Uk2JdmSZPWQ/YuS3NDt/1KS5dP2L0uyI8lFs1a0JAmYYIAkWQBcB5wBrATelmTltG7vBB6tquOAq4Erp+3/I+Avx12rJOnFJnkFciKwpaq2VtXTwPXA2dP6nA2s7dZvAk5OEoAk5wB3Axtnp1xJ0qBJBsgS4L6B7fu7tqF9qupZ4HHgsCQHAhcD79vdSZKcn2QqydT27dv3SuGSpLk7iX4pcHVV7dhdx6paU1W9quotXrx4/JVJ0jyxcILn3gYcPbC9tGsb1uf+JAuBg4DvAa8Hzk1yFXAw8FySH1bVh8ZetSQJmGyA3AGsSHIM/aA4D/jn0/qsA1YBXwTOBT5XVQX8450dklwK7DA8JGl2TSxAqurZJBcAtwALgD+pqo1JLgOmqmod8FHg40m2AI/QDxlJ0j4g/T/o54der1dTU1OTLkOS5pQkG6qqN719rk6iS5ImzACRJDUxQCRJTQwQSVITA0SS1MQAkSQ1MUAkSU0MEElSEwNEktTEAJEkNTFAJElNDBBJUhMDRJLUxACRJDUxQCRJTQwQSVITA0SS1MQAkSQ1MUAkSU0MEElSEwNEktTEAJEkNTFAJElNDBBJUhMDRJLUxACRJDUxQCRJTQwQSVITA0SS1MQAkSQ1mWiAJDk9yaYkW5KsHrJ/UZIbuv1fSrK8a39zkg1Jvt79fNOsFy9J89zEAiTJAuA64AxgJfC2JCundXsn8GhVHQdcDVzZtT8MnFlVPw+sAj4+O1VLknaa5BXIicCWqtpaVU8D1wNnT+tzNrC2W78JODlJquorVfWdrn0jcECSRbNStSQJmGyALAHuG9i+v2sb2qeqngUeBw6b1udXgTur6qkx1SlJGmLhpAsYRZLj6d/WOvUl+pwPnA+wbNmyWapMkn78TfIKZBtw9MD20q5taJ8kC4GDgO9120uBTwO/VlXf3tVJqmpNVfWqqrd48eK9WL4kzW+TDJA7gBVJjknyMuA8YN20PuvoT5IDnAt8rqoqycHAzcDqqvrfs1WwJOl5EwuQbk7jAuAW4FvAjVW1McllSc7qun0UOCzJFuBCYOdbfS8AjgP+IMld3XLELA9Bkua1VNWka5g1vV6vpqamJl2GJM0pSTZUVW96u59ElyQ1MUAkSU0MEElSEwNEktTEAJEkNTFAJElNDBBJUhMDRJLUxACRJDUxQCRJTQwQSVITA0SS1MQAkSQ1MUAkSU0MEElSEwNEktTEAJEkNTFAJElNDBBJUhMDRJLUxACRJDUxQCRJTQwQSVITA0SS1MQAkSQ1MUAkSU1mFCBJXplkv279p5OclWT/8ZYmSdqXzfQK5K+BlydZAnwWeAfwsXEVJUna9800QFJVTwJvAT5cVf8UOH58ZUmS9nUzDpAkbwDeDtzctS0YT0mSpLlgpgHyLuDdwKeramOSY4HPj60qSdI+b0YBUlV/VVVnVdWV3WT6w1X1O6OePMnpSTYl2ZJk9ZD9i5Lc0O3/UpLlA/ve3bVvSnLaqLVIkvbMTN+F9ckkP5nklcA3gG8m+f1RTpxkAXAdcAawEnhbkpXTur0TeLSqjgOuBq7sXrsSOI/+PMzpwIe740mSZslMb2GtrKongHOAvwSOof9OrFGcCGypqq1V9TRwPXD2tD5nA2u79ZuAk5Oka7++qp6qqruBLd3xJEmzZKYBsn/3uY9zgHVV9QxQI557CXDfwPb9XdvQPlX1LPA4cNgMXwtAkvOTTCWZ2r59+4glS5J2mmmA/GfgHuCVwF8neQ3wxLiK2puqak1V9aqqt3jx4kmXI0k/NmY6iX5tVS2pql+uvnuBXxrx3NuAowe2l3ZtQ/skWQgcBHxvhq+VJI3RTCfRD0ryRztvBSX5IP2rkVHcAaxIckySl9GfFF83rc86YFW3fi7wuaqqrv287l1axwArgC+PWI8kaQ/M9BbWnwDfB/5ZtzwB/OkoJ+7mNC4AbgG+BdzYfcbksiRndd0+ChyWZAtwIbC6e+1G4Ebgm8BngN+uqh+NUo8kac+k/wf9bjold1XVCbtr29f1er2ampqadBmSNKck2VBVventM70C+UGSNw4c7BeBH+yt4iRJc8/CGfb7TeC/Jjmo236U5+cmJEnz0IwCpKq+Crw2yU92208keRfwtTHWJknah+3RNxJW1RPdJ9KhP6ktSZqnRvlK2+y1KiRJc84oATLqo0wkSXPYS86BJPk+w4MiwAFjqUiSNCe8ZIBU1U/MViGSpLlllFtYkqR5zACRJDUxQCRJTQwQSVITA0SS1MQAkSQ1MUAkSU0MEElSEwNEktTEAJEkNTFAJElNDBBJUhMDRJLUxACRJDUxQCRJTQwQSVITA0SS1MQAkSQ1MUAkSU0MEElSEwNEktTEAJEkNZlIgCQ5NMn6JJu7n4fsot+qrs/mJKu6tlckuTnJ3ybZmOSK2a1ekgSTuwJZDdxWVSuA27rtF0hyKPBe4PXAicB7B4LmA1X1s8DrgF9McsbslC1J2mlSAXI2sLZbXwucM6TPacD6qnqkqh4F1gOnV9WTVfV5gKp6GrgTWDr+kiVJgyYVIEdW1QPd+oPAkUP6LAHuG9i+v2v7/5IcDJxJ/ypGkjSLFo7rwEluBV41ZNclgxtVVUmq4fgLgU8B11bV1pfodz5wPsCyZcv29DSSpF0YW4BU1Sm72pfku0mOqqoHkhwFPDSk2zbgpIHtpcDtA9trgM1Vdc1u6ljT9aXX6+1xUEmShpvULax1wKpufRXw50P63AKcmuSQbvL81K6NJJcDBwHvGn+pkqRhJhUgVwBvTrIZOKXbJkkvyUcAquoR4P3AHd1yWVU9kmQp/dtgK4E7k9yV5DcmMQhJms9SNX/u6vR6vZqampp0GZI0pyTZUFW96e1+El2S1MQAkSQ1MUAkSU0MEElSEwNEktTEAJEkNTFAJElNDBBJUhMDRJLUxACRJDUxQCRJTQwQSVITA0SS1MQAkSQ1MUAkSU0MEElSEwNEktTEAJEkNTFAJElNDBBJUhMDRJLUxACRJDUxQCRJTQwQSVITA0SS1MQAkSQ1MUAkSU0MEElSEwNEktTEAJEkNTFAJElNJhIgSQ5Nsj7J5u7nIbvot6rrsznJqiH71yX5xvgrliRNN6krkNXAbVW1Arit236BJIcC7wVeD5wIvHcwaJK8BdgxO+VKkqabVICcDazt1tcC5wzpcxqwvqoeqapHgfXA6QBJDgQuBC4ff6mSpGEmFSBHVtUD3fqDwJFD+iwB7hvYvr9rA3g/8EHgyd2dKMn5SaaSTG3fvn2EkiVJgxaO68BJbgVeNWTXJYMbVVVJag+OewLwU1X1u0mW765/Va0B1gD0er0Zn0eS9NLGFiBVdcqu9iX5bpKjquqBJEcBDw3ptg04aWB7KXA78Aagl+Qe+vUfkeT2qjoJSdKsmdQtrHXAzndVrQL+fEifW4BTkxzSTZ6fCtxSVf+xql5dVcuBNwJ/Z3hI0uybVIBcAbw5yWbglG6bJL0kHwGoqkfoz3Xc0S2XdW2SpH1AqubPtECv16upqalJlyFJc0qSDVXVm97uJ9ElSU0MEElSEwNEktTEAJEkNTFAJElNDBBJUhMDRJLUxACRJDUxQCRJTQwQSVITA0SS1MQAkSQ1MUAkSU0MEElSEwNEktTEAJEkNTFAJElNDBBJUhMDRJLUxACRJDUxQCRJTQwQSVITA0SS1MQAkSQ1SVVNuoZZk2Q7cO+k69hDhwMPT7qIWeaY5wfHPHe8pqoWT2+cVwEyFyWZqqrepOuYTY55fnDMc5+3sCRJTQwQSVITA2Tft2bSBUyAY54fHPMc5xyIJKmJVyCSpCYGiCSpiQGyD0hyaJL1STZ3Pw/ZRb9VXZ/NSVYN2b8uyTfGX/HoRhlzklckuTnJ3ybZmOSK2a1+zyQ5PcmmJFuSrB6yf1GSG7r9X0qyfGDfu7v2TUlOm9XCR9A65iRvTrIhyde7n2+a9eIbjPI77vYvS7IjyUWzVvTeUFUuE16Aq4DV3fpq4MohfQ4FtnY/D+nWDxnY/xbgk8A3Jj2ecY8ZeAXwS12flwFfAM6Y9Jh2Mc4FwLeBY7tavwqsnNbnXwP/qVs/D7ihW1/Z9V8EHNMdZ8GkxzTmMb8OeHW3/nPAtkmPZ5zjHdh/E/DfgYsmPZ49WbwC2TecDazt1tcC5wzpcxqwvqoeqapHgfXA6QBJDgQuBC4ff6l7TfOYq+rJqvo8QFU9DdwJLB1/yU1OBLZU1dau1uvpj33Q4L/FTcDJSdK1X19VT1XV3cCW7nj7uuYxV9VXquo7XftG4IAki2al6naj/I5Jcg5wN/3xzikGyL7hyKp6oFt/EDhySJ8lwH0D2/d3bQDvBz4IPDm2Cve+UccMQJKDgTOB28ZQ496w2zEM9qmqZ4HHgcNm+Np90ShjHvSrwJ1V9dSY6txbmsfb/fF3MfC+Wahzr1s46QLmiyS3Aq8asuuSwY2qqiQzfm91khOAn6qq351+X3XSxjXmgeMvBD4FXFtVW9uq1L4oyfHAlcCpk65lzC4Frq6qHd0FyZxigMySqjplV/uSfDfJUVX1QJKjgIeGdNsGnDSwvRS4HXgD0EtyD/3f5xFJbq+qk5iwMY55pzXA5qq6ZvRqx2YbcPTA9tKubVif+7tQPAj43gxfuy8aZcwkWQp8Gvi1qvr2+Msd2SjjfT1wbpKrgIOB55L8sKo+NPaq94ZJT8K4FMAf8sIJ5auG9DmU/n3SQ7rlbuDQaX2WM3cm0UcaM/35nv8B7DfpsexmnAvpT/4fw/MTrMdP6/PbvHCC9cZu/XheOIm+lbkxiT7KmA/u+r9l0uOYjfFO63Mpc2wSfeIFuBT07/3eBmwGbh34T7IHfGSg37+iP5G6Bfj1IceZSwHSPGb6f+EV8C3grm75jUmP6SXG+svA39F/p84lXdtlwFnd+svpvwNnC/Bl4NiB117SvW4T++g7zfbmmIH3AP934Pd6F3DEpMczzt/xwDHmXID4KBNJUhPfhSVJamKASJKaGCCSpCYGiCSpiQEiSWpigEgjSvKjJHcNLC96GusIx14+V56wrPnHT6JLo/tBVZ0w6SKk2eYViDQmSe5JclX33RZfTnJc1748yeeSfC3JbUmWde1HJvl0kq92yy90h1qQ5L90333y2SQHdP1/J8k3u+NcP6Fhah4zQKTRHTDtFtZbB/Y9XlU/D3wIuKZr+2NgbVX9feC/Add27dcCf1VVrwX+Ac8/3nsFcF1VHQ88Rv8ptdB/BMzruuP85niGJu2an0SXRpRkR1UdOKT9HuBNVbU1yf7Ag1V1WJKHgaOq6pmu/YGqOjzJdmBpDTy+vHvC8vqqWtFtXwzsX1WXJ/kMsAP4M+DPqmrHmIcqvYBXINJ41S7W98Tg92H8iOfnLn8FuI7+1cod3VNepVljgEjj9daBn1/s1v+G/hNZAd5O/yt5of9wyd8CSLIgyUG7OmiS/YCjq//NjBfTfzz4i66CpHHyLxZpdAckuWtg+zNVtfOtvIck+Rr9q4i3dW3/BvjTJL8PbAd+vWv/t8CaJO+kf6XxW8ADDLcA+EQXMqH/pVqP7aXxSDPiHIg0Jt0cSK+qHp50LdI4eAtLktTEKxBJUhOvQCRJTQwQSVITA0SS1MQAkSQ1MUAkSU3+H+tgRMfJynImAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n",
    "                                                 -1,\n",
    "                                                 img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([word_to_index('<start>')], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input,\n",
    "                                                         features,\n",
    "                                                         hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        predicted_word = tf.compat.as_text(index_to_word(predicted_id).numpy())\n",
    "        result.append(predicted_word)\n",
    "\n",
    "        if predicted_word == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for i in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[i], (8, 8))\n",
    "        grid_size = max(int(np.ceil(len_result/2)), 2)\n",
    "        ax = fig.add_subplot(grid_size, grid_size, i+1)\n",
    "        ax.set_title(result[i])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Caption: <start> a skier stands next to skis stuck into the snow. <end>\n",
      "Prediction Caption: sign skis areas the stands background ham, breakfast cluttered. trees fenced inside sun there towers game gold frisbee wooden eggs, grazing clock orioles tomato, large front desk. area the orioles eggs, his structure next desk. enclosed an his field. fenced covered bacon. breakfast field. front ham enclosure an enclosed at\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21756/601318523.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Real Caption:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_caption\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Prediction Caption:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mplot_attention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_plot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21756/3853974765.py\u001b[0m in \u001b[0;36mplot_attention\u001b[1;34m(image, result, attention_plot)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mtemp_att\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_plot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mgrid_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen_result\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\uvapp\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    770\u001b[0m             projection_class, pkw = self._process_projection_requirements(\n\u001b[0;32m    771\u001b[0m                 *args, **kwargs)\n\u001b[1;32m--> 772\u001b[1;33m             \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    773\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_axes_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\uvapp\\lib\\site-packages\\matplotlib\\axes\\_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \"\"\"\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# _axes_class is set in the subplot_class_factory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_axes_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;31m# This will also update the axes position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_subplotspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSubplotSpec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_from_subplot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\uvapp\\lib\\site-packages\\matplotlib\\_api\\deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    454\u001b[0m                 \u001b[1;34m\"parameter will become keyword-only %(removal)s.\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m                 name=name, obj_type=f\"parameter of {func.__name__}()\")\n\u001b[1;32m--> 456\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m     \u001b[1;31m# Don't modify *func*'s signature, as boilerplate.py needs it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\uvapp\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fig, rect, facecolor, frameon, sharex, sharey, label, xscale, yscale, box_aspect, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rasterization_zorder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcla\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m         \u001b[1;31m# funcs used to format x and y - fall back on major formatters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\uvapp\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36mcla\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1298\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_axis_on\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1300\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1301\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\uvapp\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mset_clip_path\u001b[1;34m(self, clippath, transform)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclippath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclippath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    936\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmajorTicks\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminorTicks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m             \u001b[0mchild\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclippath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\uvapp\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mset_clip_path\u001b[1;34m(self, path, transform)\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRectangle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m                 self.clipbox = TransformedBbox(Bbox.unit(),\n\u001b[1;32m--> 790\u001b[1;33m                                                path.get_transform())\n\u001b[0m\u001b[0;32m    791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clippath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m                 \u001b[0msuccess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\uvapp\\lib\\site-packages\\matplotlib\\patches.py\u001b[0m in \u001b[0;36mget_transform\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[1;34m\"\"\"Return the `~.transforms.Transform` applied to the `Patch`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_patch_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mArtist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_data_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\uvapp\\lib\\site-packages\\matplotlib\\patches.py\u001b[0m in \u001b[0;36mget_patch_transform\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[0mbbox\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_bbox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         return (transforms.BboxTransformTo(bbox)\n\u001b[1;32m--> 754\u001b[1;33m                 + transforms.Affine2D().rotate_deg_around(\n\u001b[0m\u001b[0;32m    755\u001b[0m                     bbox.x0, bbox.y0, self.angle))\n\u001b[0;32m    756\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\uvapp\\lib\\site-packages\\matplotlib\\transforms.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, matrix, **kwargs)\u001b[0m\n\u001b[0;32m   1912\u001b[0m             \u001b[1;31m# A bit faster than np.identity(3).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m             \u001b[0mmatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIdentityTransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1914\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mtx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1915\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# captions on the validation set\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_caption = ' '.join([tf.compat.as_text(index_to_word(i).numpy())\n",
    "                         for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "print('Real Caption:', real_caption)\n",
    "print('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image, result, attention_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a0855005469a133b1a4fed7e6a9d3300657ffc4061414113a9f065e922aa0ee4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('uvapp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
