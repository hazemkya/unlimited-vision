{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# your model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"config.ini\")\n",
    "\n",
    "#importing local module \n",
    "from models.subclasses import *\n",
    "from models.utilities import *\n",
    "from models.train_utils import *\n",
    "from models.predict import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train sample size (-1 for max) \n",
    "# can't exceed 118286 sample\n",
    "sample = int(config['config']['train_sample'])\n",
    "\n",
    "#train split percentage 80-20\n",
    "percentage = float(config['config']['percentage'])\n",
    "\n",
    "# Max word count for a caption.\n",
    "max_length = int(config['config']['max_length'])\n",
    "# Use the top words for a vocabulary.\n",
    "vocabulary_size = int(config['config']['vocabulary_size'])\n",
    "use_glove = bool(config['config']['use_glove'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data lists\n",
    "# import data and save it to a dict, also save it's keys in a list \n",
    "train_image_paths, image_path_to_caption = import_files(shuffle= False, method = \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepair data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions = []\n",
    "img_name_vector = []\n",
    "for image_path in train_image_paths:\n",
    "  caption_list = image_path_to_caption[image_path]\n",
    "  train_captions.extend(caption_list)\n",
    "  img_name_vector.extend([image_path] * len(caption_list))\n",
    "\n",
    "print(train_captions[0])\n",
    "Image.open(img_name_vector[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepair the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and freeze feature extractor model\n",
    "image_features_extract_model = get_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, tokenizer, cap_vector = tokenization(train_captions, max_length, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = \"./dataset/glove.6B/glove.6B.100d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(glove_path) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = tokenizer.get_vocabulary()\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(vocabulary) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_train, cap_train = split_data(img_name_vector, cap_vector ,image_features_extract_model,  percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(img_name_train, cap_train, tokenizer.get_vocabulary() , train_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these parameters according to your system's configuration\n",
    "BATCH_SIZE = int(config['config']['BATCH_SIZE'])\n",
    "BUFFER_SIZE = int(config['config']['BUFFER_SIZE'])\n",
    "\n",
    "if use_glove:\n",
    "    embedding_dim = 100\n",
    "else:\n",
    "    embedding_dim = int(config['config']['embedding_dim'])\n",
    "\n",
    "units = int(config['config']['units'])\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = int(config['config']['features_shape'])\n",
    "attention_features_shape = int(config['config']['attention_features_shape'])\n",
    "\n",
    "# Training parameters\n",
    "epochs = int(config['config']['epochs'])\n",
    "num_steps = len(img_name_train) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = make_dataset(img_name_train, cap_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "if use_glove:\n",
    "    decoder = RNN_Decoder(embedding_dim, units, num_tokens, embedding_matrix)\n",
    "else:\n",
    "    decoder = RNN_Decoder(embedding_dim, units, tokenizer.vocabulary_size(), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "  # restoring the latest checkpoint in checkpoint_path\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if start_epoch == 0:\n",
    "    loss_plot = []\n",
    "else:\n",
    "    loss_plot = load_loss()\n",
    "\n",
    "train(epochs, start_epoch, ckpt_manager,\n",
    "          num_steps, dataset, decoder,\n",
    "          encoder, word_to_index, loss_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions on the validation set\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_caption = ' '.join([tf.compat.as_text(index_to_word(i).numpy())\n",
    "                         for i in cap_val[rid] if i not in [0]])\n",
    "\n",
    "result, attention_plot = evaluate(image, encoder, decoder, image_features_extract_model,\n",
    "                                    word_to_index, index_to_word)\n",
    "\n",
    "print('Real Caption:', real_caption)\n",
    "print('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image, result, attention_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a0855005469a133b1a4fed7e6a9d3300657ffc4061414113a9f065e922aa0ee4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('uvapp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
